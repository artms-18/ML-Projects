{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pets_Object_Localization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjbBzQ6fGPz+2W9+jVAqcE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artms-18/ML-Projects/blob/main/Pets_Object_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpL8R5Pshe0t"
      },
      "source": [
        "## Using a Resnet50 feature extractor to classify and locate objects within the Oxford-IIIT Pet Dataset\n",
        "\n",
        "In this notebook, we will achieve this by implementing the following steps:\n",
        "1. Importing Modules\n",
        "2. Preprocessing data\n",
        "3. Defining helper functions to later view the data\n",
        "4. Choosing a runtime strategy\n",
        "4. Creating the model using a pretrained feature extractor, but not including the top so that we can add our own Dense layers\n",
        "5. Compiling and fitting the model\n",
        "6. Visualizing the results and looking at accuracy as well as loss on Tensorflow Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJxJwDfulsUV"
      },
      "source": [
        "## Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqjZGv5h8by"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import zipfile\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyfPy-hclvGW"
      },
      "source": [
        "## Getting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDcXPfv8iyF8"
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "awerAqXqj6lC",
        "outputId": "04a174b7-bf86-4705-c15c-7c0164c9a62d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-03e4250f-994a-434a-93ac-2902e790f5b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-03e4250f-994a-434a-93ac-2902e790f5b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mikashaw\",\"key\":\"5ea1167847d0f4828837c42bb91725e5\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsxAtNc1kAj4",
        "outputId": "13a96fed-fd3c-45fc-e4fc-cf7362f24fcb"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle datasets download -d devdgohil/the-oxfordiiit-pet-dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading the-oxfordiiit-pet-dataset.zip to /content\n",
            " 99% 775M/780M [00:24<00:00, 53.6MB/s]\n",
            "100% 780M/780M [00:24<00:00, 32.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQJUxRBBkXgf"
      },
      "source": [
        "with zipfile.ZipFile('/content/the-oxfordiiit-pet-dataset.zip', 'r') as zip_file:\n",
        "  zip_file.extractall('/content/the-oxfordiiit-pet-dataset')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMZzBTg_lVRb"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKKQ5b4sl0E7"
      },
      "source": [
        "Right now, the annotations are in an xml file. In order to train the model, we will need to convert the data contained into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ItlsZjmGQz"
      },
      "source": [
        "XML_PATH = '/content/the-oxfordiiit-pet-dataset/annotations/annotations/xmls'\n",
        "SPLIT_RATIO = 0.8 #while splitting into training and validation"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_rQe0e3mYsP"
      },
      "source": [
        "class_names = {} #storing the name pertaining to each class of images\n",
        "k = 0 #classes will be label encoded\n",
        "output = [] #this will soon be a list containing the labels for each image (paht, height, width, xmin, xmas, ymax, class_name, class_names[class_name])\n",
        "\n",
        "xml_files = glob.glob(\"{}/*xml\".format(XML_PATH))\n",
        "for i, xml_file in enumerate(xml_files):\n",
        "  tree = ET.parse(xml_file)\n",
        "  path = os.path.join(XML_PATH, tree.findtext(\"./filename\"))\n",
        "\n",
        "  height = int(tree.findtext(\"./size/height\"))\n",
        "  width = int(tree.findtext(\"./size/width\"))\n",
        "  xmin = int(tree.findtext(\"./object/bndbox/xmin\"))\n",
        "  ymin = int(tree.findtext(\"./object/bndbox/ymin\"))\n",
        "  xmax = int(tree.findtext(\"./object/bndbox/xmax\"))\n",
        "  ymax = int(tree.findtext(\"./object/bndbox/ymax\"))\n",
        "\n",
        "  basename = os.path.basename(path)\n",
        "  basename = os.path.splitext(basename)[0]\n",
        "  class_name = basename[:basename.rfind('_')].lower() #gets the lowercased name of pet (getting rid of jpg number)\n",
        "  if class_name not in class_names:\n",
        "    class_names[class_name] = k\n",
        "    k+=1\n",
        "\n",
        "  output.append((path, height, width, xmin, ymin, xmax, ymax, class_name, class_names[class_name]))\n",
        "\n",
        "output.sort(key = lambda tup: tup[-1]) #sorting by class\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MM00Y_PtmtX",
        "outputId": "72f117cb-1d1e-4183-da34-448ccc0a6184"
      },
      "source": [
        "#taking a look at the amounts of images in each image class\n",
        "\n",
        "lengths = []\n",
        "i = 0\n",
        "last = 0\n",
        "for j, row in enumerate(output):\n",
        "  if last == row[-1]: #since earlier, we sorted 'output' from class 0 increasing, we can traverse the list much easier\n",
        "    i += 1 #this will continue happening until last IS NOT equal to the class label\n",
        "  else: \n",
        "    print(f\"class {output[j-1][-2]}: {i} images\") #we are doing j-1 because this happens ONLY when we move on to the next class\n",
        "    lengths.append(i)\n",
        "    i = 1\n",
        "    last += 1\n",
        "\n",
        "print(f\"class {output[j-1][-2]}: {i} images\") #since the last one still needs to be expressed\n",
        "lengths.append(i)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class yorkshire_terrier: 100 images\n",
            "class sphynx: 100 images\n",
            "class saint_bernard: 99 images\n",
            "class great_pyrenees: 100 images\n",
            "class japanese_chin: 100 images\n",
            "class english_setter: 100 images\n",
            "class chihuahua: 100 images\n",
            "class staffordshire_bull_terrier: 100 images\n",
            "class german_shorthaired: 100 images\n",
            "class persian: 100 images\n",
            "class abyssinian: 99 images\n",
            "class boxer: 100 images\n",
            "class newfoundland: 100 images\n",
            "class american_pit_bull_terrier: 100 images\n",
            "class basset_hound: 100 images\n",
            "class keeshond: 100 images\n",
            "class havanese: 100 images\n",
            "class ragdoll: 99 images\n",
            "class bengal: 98 images\n",
            "class birman: 100 images\n",
            "class english_cocker_spaniel: 100 images\n",
            "class leonberger: 100 images\n",
            "class shiba_inu: 100 images\n",
            "class miniature_pinscher: 100 images\n",
            "class wheaten_terrier: 100 images\n",
            "class egyptian_mau: 92 images\n",
            "class beagle: 100 images\n",
            "class british_shorthair: 100 images\n",
            "class bombay: 100 images\n",
            "class american_bulldog: 100 images\n",
            "class pomeranian: 100 images\n",
            "class maine_coon: 100 images\n",
            "class samoyed: 99 images\n",
            "class russian_blue: 100 images\n",
            "class scottish_terrier: 100 images\n",
            "class siamese: 100 images\n",
            "class pug: 100 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaR4kavUwkBL"
      },
      "source": [
        "## Splitting into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITWDtHR6zA1x",
        "outputId": "1e370960-6cbe-448a-9fa3-c1edd71e315e"
      },
      "source": [
        "training_data = []\n",
        "validation_data = []\n",
        "s = 0\n",
        "\n",
        "for c in lengths:\n",
        "  for i in range(c):\n",
        "    path, height, width, xmin, ymin, xmax, ymax, class_name, class_id = output[s]\n",
        "    if xmin >= xmax or ymin > ymax or xmax > width or ymax > height or xmin < 0 or ymin < 0:\n",
        "      print(f\"Warning: {path} contains invalid box. Skipped...\")\n",
        "      continue\n",
        "\n",
        "    if i <= c * SPLIT_RATIO:\n",
        "      training_data.append(output[s])\n",
        "\n",
        "    else:\n",
        "      validation_data.append(output[s])\n",
        "\n",
        "    s+= 1\n",
        "print(len(training_data))\n",
        "print(len(validation_data))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2984\n",
            "702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SAqCwNv0Cyx",
        "outputId": "a28329be-98b7-4b4e-f07a-2e6ee55e0046"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/the-oxfordiiit-pet-dataset/annotations/annotations/xmls/yorkshire_terrier_177.jpg',\n",
              " 500,\n",
              " 375,\n",
              " 76,\n",
              " 89,\n",
              " 287,\n",
              " 210,\n",
              " 'yorkshire_terrier',\n",
              " 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziqB4IiMkRu-"
      },
      "source": [
        "## Getting Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj3hHLCTk5Cm"
      },
      "source": [
        "IMG_PATH = '/content/the-oxfordiiit-pet-dataset/images/images'\n",
        "IMG_PATHS = []\n",
        "img_files = glob.glob(\"{}/*jpg\".format(IMG_PATH)) #getting all the jpg files"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x_FRhC1tpT0X",
        "outputId": "16b33982-d16f-4f20-b2cc-67a4a2adf44c"
      },
      "source": [
        "img_files[0].split('/')[-1].split('.')[-2]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great_pyrenees_59'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwOEES6rolay"
      },
      "source": [
        "def get_image_and_label(img_files, labels):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    Args: 2 lists - one containing the img file paths and the other containing the preprocessed labels\n",
        "    Returns: a tuple containing an img with its respective label\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  images_and_labels = []\n",
        "\n",
        "  for i in range(len(img_files)):\n",
        "    img = img_files[i].split('/')[-1].split('.')[-2]\n",
        "    for j in range(len(labels)):\n",
        "      label = labels[j][0].split('/')[-1].split('.')[-2]\n",
        "      if img == label:\n",
        "        images_and_labels.append((img_files[i], labels[j]))\n",
        "        break\n",
        "   \n",
        "  return images_and_labels\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQikTDnRq0La"
      },
      "source": [
        "ims_and_labels_training = get_image_and_label(img_files, training_data)\n",
        "img_and_labels_validation = get_image_and_label(img_files, validation_data)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psi3Ndob4msK",
        "outputId": "f8e62768-7722-401a-96c6-b07866961a7d"
      },
      "source": [
        "ims_and_labels_training[0]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/the-oxfordiiit-pet-dataset/images/images/Sphynx_117.jpg',\n",
              " ('/content/the-oxfordiiit-pet-dataset/annotations/annotations/xmls/Sphynx_117.jpg',\n",
              "  500,\n",
              "  334,\n",
              "  143,\n",
              "  154,\n",
              "  237,\n",
              "  250,\n",
              "  'sphynx',\n",
              "  1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCyei24nuPRu"
      },
      "source": [
        "temp_image = img_files[0]\n",
        "temp_image = cv2.imread(temp_image)\n",
        "temp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB)\n",
        "temp_image = cv2.resize(temp_image, (256,256))\n",
        "image_tensor = tf.convert_to_tensor(temp_image, dtype = tf.float32)\n",
        "image_tensor = tf.expand_dims(image_tensor, 0)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMKtGelynSE5"
      },
      "source": [
        "#preprocessing functions\n",
        "\n",
        "def convert_to_tensor(img_path):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    Args: filepath to an image\n",
        "    Returns: a resized RBB image with a batch dimension in the form of a tensor of dtype tf.float32\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RBG)\n",
        "  img = cv2.resize(img, (256,256))\n",
        "  image_tensor = tf.convert_to_tensor(img, dtype = tf.float32)\n",
        "  image_tensor = tf.expand_dims(image_tensor, 0)\n",
        "  return image_tensor\n",
        "\n",
        "def preprocess_label(label):\n",
        "\n",
        "  \"\"\"\n",
        "    Args: A label containing a tuple with values (path, height, width, xmin, ymin, xmax, ymax, class_name, class_names[class_name])\n",
        "    Returns: A label containinga tuple with values (class_names[class_name], [xmin, ymin, xmax, ymax])\n",
        "\n",
        "    **NOTE** the class_names[class_name] will be one hot encoded\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  path, height, width, xmin, ymin, xmax, ymax, class_name, class_label = label\n",
        "\n",
        "  label = tf.one_hot(class_label, 100)\n",
        "  b_box = [xmin, ymin, xmax, ymax]\n",
        "\n",
        "  return (label, b_box)\n",
        "\n",
        "def preprocessing(image_and_label):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "    Args: a list containing tuples of list[0] = image path, list[2] = outputs/labels\n",
        "    Returns: a tuple containing the preprocessed images and labels\n",
        "\n",
        "    ##MIKA FIGURE OUT HOW THE LABEL SHOULD BE FORMATTED\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  image, label = image_and_label\n",
        "  preprocessed_image = convert_to_tensor(image)\n",
        "  preprocessed_label = preprocess_label(label)\n",
        "\n",
        "  return (preprocessed_image, preprocessed_label)\n",
        "\n",
        "def create_datasets(images_and_labels):\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  for image_and_label in image_and_labels:\n",
        "    image, label = preprocessing(image_and_label)\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "  dataset = dataset.shuffle(100, reshuffle_each_iteration = True)\n",
        "  dataset = dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xm5KVJ76iEB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}